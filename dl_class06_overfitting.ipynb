{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Course: Deep Learning\n",
        "# Author: Sandro Camargo <sandrocamargo@unipampa.edu.br>\n",
        "# Non Linear Regression with Multi Layer Perceptron Example\n",
        "# Overfitting Example\n",
        "# Dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names"
      ],
      "metadata": {
        "id": "XwMSuk2dgEoL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fCWMrgQpht0"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import keras # Neural Network Library\n",
        "import pandas as pd # Data Manipulation library\n",
        "import numpy as np # Fast Numeric Computing library\n",
        "import tensorflow as tf # Optimizers\n",
        "import matplotlib.pyplot as plt # Plot library\n",
        "from keras import layers # Layers to a neural network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset\n",
        "colnames=['Crim','Zn','Indus','Chas','Nox','Rm','Age','Dis','Rad','Tax','PtRatio','B','Lstat','MedV']\n",
        "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data', sep=\"\\s+\", header=None, names=colnames)\n",
        "\n",
        "# inspecting columns and data types from \"data\" dataframe\n",
        "data.info()"
      ],
      "metadata": {
        "id": "sab2uBzLpn5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive Statistics\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "_lSgns1VIMjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split inputs and outputs\n",
        "output = data['MedV']\n",
        "inputs = data\n",
        "inputs.drop(inputs.columns[[13]], axis=1, inplace=True) # column 13 is removed, because it is the output (y)"
      ],
      "metadata": {
        "id": "6KU1-1_wIR6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining normalizing function\n",
        "def normalize(dataset):\n",
        "  mu = np.mean(dataset, axis = 0)\n",
        "  sigma = np.std(dataset, axis = 0)\n",
        "  return (dataset - mu)/sigma"
      ],
      "metadata": {
        "id": "Rb3tu8L-pzgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_norm = normalize(inputs)\n",
        "\n",
        "np.random.seed(1) # Random numbers will be ever the same\n",
        "rnd = np.random.rand(len(inputs_norm)) < 0.8 # Training set will contain 80% of the data\n",
        "\n",
        "# Creating the training dataset (80%)\n",
        "train_x = inputs_norm[rnd]\n",
        "train_y = output[rnd]\n",
        "\n",
        "# Creating the validation dataset (20%)\n",
        "val_x = inputs_norm[~rnd]\n",
        "val_y = output[~rnd]\n",
        "\n",
        "# Verifying dataset dimensions\n",
        "print('The training dataset (inputs) dimensions are: ', train_x.shape)\n",
        "print('The training dataset (outputs) dimensions are: ', train_y.shape)\n",
        "print('The validation dataset (inputs) dimensions are: ', val_x.shape)\n",
        "print('The validation dataset (outputs) dimensions are: ', val_y.shape)"
      ],
      "metadata": {
        "id": "D0uNu7FSp4ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_train_model_nlayers(data_train_norm, labels_train, data_val_norm, labels_val, num_neurons, num_layers):\n",
        "  # build model\n",
        "  # input layer\n",
        "  inputs = keras.Input(shape = data_train_norm.shape[1])\n",
        "  # he initialization\n",
        "  initializer = tf.keras.initializers.HeNormal()\n",
        "  # first hidden layer\n",
        "  dense = layers.Dense(num_neurons, activation = 'relu', kernel_initializer = initializer)(inputs)\n",
        "  # customized number of layers and neurons per layer\n",
        "  for i in range(num_layers - 1):\n",
        "    dense = layers.Dense(num_neurons, activation = 'relu', kernel_initializer = initializer)(dense)\n",
        "\n",
        "  # output layer\n",
        "  outputs = layers.Dense(1)(dense)\n",
        "  model = keras.Model(inputs = inputs, outputs = outputs, name = 'model')\n",
        "  # set optimizer and loss\n",
        "  opt = keras.optimizers.Adam(learning_rate = 0.001)\n",
        "  model.compile(loss = 'mse', optimizer = opt, metrics = ['mse','mae'])\n",
        "  # train model\n",
        "  history = model.fit(\n",
        "    data_train_norm, labels_train,\n",
        "    epochs = 10000, verbose = 1,\n",
        "    batch_size = data_train_norm.shape[0],\n",
        "    validation_data = (data_val_norm, labels_val))\n",
        "  # save performances\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  return hist, model"
      ],
      "metadata": {
        "id": "s59a-xH4qJOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist, model = create_and_train_model_nlayers(train_x, train_y, val_x, val_y, 20, 4)"
      ],
      "metadata": {
        "id": "Gf64fh3-rBXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "c1Juv5Brp1PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot should be generated just to inspect the learning convergence.\n",
        "It is expected a decreasing of the loss function value through the epochs."
      ],
      "metadata": {
        "id": "NzwVwUfLh2l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist['mse'], label=\"Training MSE\")\n",
        "plt.plot(hist['val_mse'], label=\"Validation MSE\")\n",
        "plt.title('Training Process')\n",
        "plt.ylabel('Loss Function (MSE)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig(\"mlp-regr-regularization.png\")\n",
        "plt.show()\n",
        "print(\"Training Categorical Accuracy:\", round(history.history['categorical_accuracy'][-1],4))\n",
        "print(\"Validation Categorical Accuracy:\", round(history.history['val_categorical_accuracy'][-1],4))"
      ],
      "metadata": {
        "id": "s1KJlR9GsFAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the previous plot, but highlighting the divergence between training and validation sets"
      ],
      "metadata": {
        "id": "Ho4j0W5ziY7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist['mse'], label=\"Training MSE\")\n",
        "plt.plot(hist['val_mse'], label=\"Validation MSE\")\n",
        "plt.ylim(0,30)\n",
        "plt.title('Training Process')\n",
        "plt.ylabel('Loss Function (MSE)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper center')\n",
        "plt.savefig(\"mlp-regr-regularization-zoom.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XlgnyEz1w2Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = model.predict(val_x) # predict radon activities with the built linear regression model\n",
        "\n",
        "plt.scatter(val_y, test_predictions, marker = 'o', c = 'blue')\n",
        "plt.plot([0,55], [0,55], color = 'black', ls = '--')\n",
        "plt.ylabel('Predictions')\n",
        "plt.xlabel('Real Values')\n",
        "plt.title('Regression with MLP (Testing Set)')\n",
        "plt.ylim(0, 55)\n",
        "plt.xlim(0, 55)\n",
        "plt.axis(True)\n",
        "plt.savefig(\"mlp-regr-regul-testing.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Correlation Coefficient in testing set: %.4f\" % np.corrcoef(np.transpose(test_predictions), np.transpose(val_y))[0,1])"
      ],
      "metadata": {
        "id": "Y11F1ecPyBK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions = model.predict(train_x) # predict radom activities with the built linear regression model\n",
        "\n",
        "plt.scatter(train_y, train_predictions, marker = 'o', c = 'blue')\n",
        "plt.plot([0,55], [0,55], color = 'black', ls = '--')\n",
        "plt.ylabel('Predictions')\n",
        "plt.xlabel('Real Values')\n",
        "plt.title('Regression with MLP (Training Set)')\n",
        "plt.ylim(0, 55)\n",
        "plt.xlim(0, 55)\n",
        "plt.axis(True)\n",
        "plt.savefig(\"mlp-regr-regul-training.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Correlation Coefficient in training set: %.4f\" % np.corrcoef(np.transpose(train_predictions), np.transpose(train_y))[0,1])"
      ],
      "metadata": {
        "id": "_x-9EUwVyEyq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}