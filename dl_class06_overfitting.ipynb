{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Course: Deep Learning\n",
        "# Author: Sandro Camargo <sandrocamargo@unipampa.edu.br>\n",
        "\n",
        " Overfitting Example\n",
        "\n",
        " Dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names\n",
        "\n",
        "To open this code in your Google Colab environment, [click here](https://colab.research.google.com/github/Sandrocamargo/deep-learning/blob/master/dl_class06_overfitting.ipynb)."
      ],
      "metadata": {
        "id": "XwMSuk2dgEoL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fCWMrgQpht0"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import keras # Neural Network Library\n",
        "import pandas as pd # Data Manipulation library\n",
        "import numpy as np # Fast Numeric Computing library\n",
        "import tensorflow as tf # Optimizers\n",
        "import matplotlib.pyplot as plt # Plot library\n",
        "from keras import layers # Layers to a neural network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset\n",
        "colnames=['Crim','Zn','Indus','Chas','Nox','Rm','Age','Dis','Rad','Tax','PtRatio','B','Lstat','MedV']\n",
        "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data', sep=r\"\\s+\", header=None, names=colnames)\n",
        "\n",
        "# inspecting columns and data types from \"data\" dataframe\n",
        "data.info()"
      ],
      "metadata": {
        "id": "sab2uBzLpn5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive Statistics\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "_lSgns1VIMjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining normalizing function\n",
        "def normalize(dataset):\n",
        "  mu = np.mean(dataset, axis = 0)\n",
        "  sigma = np.std(dataset, axis = 0)\n",
        "  return (dataset - mu)/sigma\n",
        "\n",
        "data = normalize(data)"
      ],
      "metadata": {
        "id": "Rb3tu8L-pzgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split inputs and outputs\n",
        "output = data['MedV']\n",
        "inputs = data\n",
        "inputs.drop(inputs.columns[[13]], axis=1, inplace=True) # column 13 is removed, because it is the output (y)"
      ],
      "metadata": {
        "id": "6KU1-1_wIR6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1) # Random numbers will be ever the same\n",
        "rnd = np.random.rand(len(inputs)) < 0.8 # Training set will contain 80% of the data\n",
        "\n",
        "# Creating the training dataset (80%)\n",
        "train_x = inputs[rnd]\n",
        "train_y = output[rnd]\n",
        "\n",
        "# Creating the validation dataset (20%)\n",
        "val_x = inputs[~rnd]\n",
        "val_y = output[~rnd]\n",
        "\n",
        "# Verifying dataset dimensions\n",
        "print('The training dataset (inputs) dimensions are: ', train_x.shape)\n",
        "print('The training dataset (outputs) dimensions are: ', train_y.shape)\n",
        "print('The validation dataset (inputs) dimensions are: ', val_x.shape)\n",
        "print('The validation dataset (outputs) dimensions are: ', val_y.shape)"
      ],
      "metadata": {
        "id": "D0uNu7FSp4ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_nlayers(input_dim, num_neurons=20, num_layers=4):\n",
        "    \"\"\"\n",
        "    Cria um modelo MLP com número customizado de camadas e neurônios.\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): Número de features da entrada.\n",
        "        num_neurons (int): Número de neurônios por camada oculta.\n",
        "        num_layers (int): Número total de camadas ocultas.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: Modelo compilado.\n",
        "    \"\"\"\n",
        "    # Input layer\n",
        "    inputs = keras.Input(shape=(input_dim,))\n",
        "\n",
        "    # He initialization\n",
        "    initializer = tf.keras.initializers.HeNormal()\n",
        "\n",
        "    # Primeira camada oculta\n",
        "    x = layers.Dense(num_neurons, activation='relu', kernel_initializer=initializer)(inputs)\n",
        "\n",
        "    # Camadas ocultas adicionais\n",
        "    for _ in range(num_layers - 1):\n",
        "        x = layers.Dense(num_neurons, activation='relu', kernel_initializer=initializer)(x)\n",
        "\n",
        "    # Camada de saída\n",
        "    outputs = layers.Dense(1)(x)\n",
        "\n",
        "    # Modelo\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name='mlp_model')\n",
        "\n",
        "    # Compilação\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='mse',\n",
        "        metrics=['mse']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "s59a-xH4qJOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar modelo usando o shape do train_x\n",
        "model = create_model_nlayers(input_dim=train_x.shape[1], num_neurons=20, num_layers=4)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "c1Juv5Brp1PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 2000\n",
        "# train model\n",
        "\n",
        "history = model.fit(\n",
        "  train_x, train_y,\n",
        "  epochs = EPOCHS, verbose = 1,\n",
        "  batch_size = train_x.shape[0],\n",
        "  validation_data = (val_x, val_y))\n",
        "\n",
        "# save performances\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch"
      ],
      "metadata": {
        "id": "03WUM9nkXn5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot should be generated just to inspect the learning convergence.\n",
        "It is expected a decreasing of the loss function value through the epochs."
      ],
      "metadata": {
        "id": "NzwVwUfLh2l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist['mse'], label=\"Training MSE\")\n",
        "plt.plot(hist['val_mse'], label=\"Validation MSE\")\n",
        "plt.title('Training Process')\n",
        "plt.ylabel('Loss Function (MSE)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig(\"mlp-regr-regularization.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s1KJlR9GsFAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the previous plot, but highlighting the divergence between training and validation sets"
      ],
      "metadata": {
        "id": "Ho4j0W5ziY7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist['mse'], label=\"Training MSE\")\n",
        "plt.plot(hist['val_mse'], label=\"Validation MSE\")\n",
        "plt.ylim(0,0.5)\n",
        "plt.title('Training Process')\n",
        "plt.ylabel('Loss Function (MSE)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig(\"mlp-regr-regularization-zoom.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XlgnyEz1w2Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = model.predict(val_x) # predict radon activities with the built linear regression model\n",
        "\n",
        "plt.scatter(val_y, test_predictions, marker = 'o', c = 'blue')\n",
        "plt.plot([-5,5], [-5,5], color = 'black', ls = '--')\n",
        "plt.ylabel('Predictions')\n",
        "plt.xlabel('Real Values')\n",
        "plt.title('Regression with MLP (Testing Set)')\n",
        "plt.xlim(val_y.min()*1.15, val_y.max()*1.15)\n",
        "plt.ylim(test_predictions.min()*1.15, test_predictions.max()*1.15)\n",
        "plt.axis(True)\n",
        "plt.savefig(\"mlp-regr-regul-testing.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Correlation Coefficient in testing set: %.4f\" % np.corrcoef(np.transpose(test_predictions), np.transpose(val_y))[0,1])"
      ],
      "metadata": {
        "id": "Y11F1ecPyBK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions = model.predict(train_x) # predict radom activities with the built linear regression model\n",
        "\n",
        "plt.scatter(train_y, train_predictions, marker = 'o', c = 'blue')\n",
        "plt.plot([-5,5], [-5,5], color = 'black', ls = '--')\n",
        "plt.ylabel('Predictions')\n",
        "plt.xlabel('Real Values')\n",
        "plt.title('Regression with MLP (Training Set)')\n",
        "plt.xlim(train_y.min()*1.15, train_y.max()*1.15)\n",
        "plt.ylim(train_predictions.min()*1.15, train_predictions.max()*1.15)\n",
        "plt.axis(True)\n",
        "plt.savefig(\"mlp-regr-regul-training.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Correlation Coefficient in training set: %.4f\" % np.corrcoef(np.transpose(train_predictions), np.transpose(train_y))[0,1])"
      ],
      "metadata": {
        "id": "_x-9EUwVyEyq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}