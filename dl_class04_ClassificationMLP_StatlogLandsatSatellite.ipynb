{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9feqnHODNlEV"
      },
      "source": [
        "# Course: Deep Learning\n",
        "# Author: Sandro Camargo <sandrocamargo@unipampa.edu.br>\n",
        "# Classification with Multi Layer Perceptron Example\n",
        "# Dataset: https://archive.ics.uci.edu/ml/datasets/Statlog+%28Landsat+Satellite%29"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVnjeeftXQl6"
      },
      "source": [
        "A Python library is a collection of related functions. A library contains bundles of encapsuated code which can be used repeatedly in different programs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt7-B5mnWZRb"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import keras # Neural Network Library\n",
        "from keras import layers # Layers to a neural network\n",
        "from keras import optimizers # optimizers\n",
        "import pandas as pd # Data Manipulation library\n",
        "import numpy as np # Fast Numeric Computing library\n",
        "import tensorflow as tf # Optimizers\n",
        "import matplotlib.pyplot as plt # Plot library\n",
        "from sklearn.preprocessing import MinMaxScaler, label_binarize\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBNiKzKsWtW9"
      },
      "outputs": [],
      "source": [
        "# Loading training dataset\n",
        "colnames=['r1','r2','r3','r4','r5','r6','r7','r8','r9','g1','g2','g3','g4','g5','g6','g7','g8','g9','b1','b2','b3','b4','b5','b6','b7','b8','b9','a1','a2','a3','a4','a5','a6','a7','a8','a9','class']\n",
        "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn', header=None, delimiter=\" \", names=colnames)\n",
        "# About the parameters\n",
        "# Header=1: column names (day, month, year, ...) are in the line 1 of this CSV file.\n",
        "# skiprows=[124,125,126,170]: this lines, which not contains valid data, are not imported. If this parameter is missing, all lines are imported.\n",
        "# usecols=list(range(0,13)): The last column, which is named Classes, is not imported. If this parameter is missing, all columns are imported.\n",
        "\n",
        "# inspecting columns and data types from \"data\" dataframe\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = data.iloc[:,36]\n",
        "scaler = MinMaxScaler()\n",
        "print(scaler.fit(data))\n",
        "MinMaxScaler()\n",
        "data = pd.DataFrame(scaler.transform(data))"
      ],
      "metadata": {
        "id": "I2gEa1Jtq7Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmq8UTaXdFtF"
      },
      "source": [
        "The dataset must be randomly splitted in two parts: training set and testing set. The main approaches to split are holdout and n-fold cross validation.\n",
        "*   Training set is used for building (training) the model.\n",
        "*   Testing set is used for testing the generalization ability of the model built.\n",
        "\n",
        "Moreover, inputs($x$) and outputs($y$) must be splitted in each set.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vjSR2UWY3gi"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1) # Random numbers will be ever the same\n",
        "rnd = np.random.rand(len(data)) < 0.8 # Training set will contain 80% of the data\n",
        "\n",
        "# Creating the training dataset (80%)\n",
        "train_x = data[rnd]\n",
        "train_x.drop(train_x.columns[[36]], axis=1, inplace=True) # column 12 is removed, because it is the output (y)\n",
        "train_y = data[rnd]\n",
        "train_y_bin = label_binarize(classes[rnd], classes=[1, 2, 3, 4, 5, 6, 7])\n",
        "\n",
        "# Creating the testing dataset (20%)\n",
        "test_x = data[~rnd]\n",
        "test_x.drop(test_x.columns[[36]], axis=1, inplace=True)\n",
        "test_y = data[~rnd]\n",
        "test_y_bin = label_binarize(classes[~rnd], classes=[1, 2, 3, 4, 5, 6, 7])\n",
        "\n",
        "# Verifying dataset dimensions\n",
        "print('The training dataset (inputs) dimensions are: ', train_x.shape)\n",
        "print('The training dataset (outputs) dimensions are: ', train_y.shape)\n",
        "print('The testing dataset (inputs) dimensions are: ', test_x.shape)\n",
        "print('The testing dataset (outputs) dimensions are: ', test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8NAHkCSh5sr"
      },
      "source": [
        "After creating the datasets, the next step is defining the architecture of our model.\n",
        "\n",
        "It must be defined:\n",
        "\n",
        "\n",
        "*   Architecture: in terms of neurons and layers\n",
        "*   Optimizer: is the algorithm or method used to change the weights in order to minimize the loss function.\n",
        "\n",
        "The last step is compiling the model. In this step the loss function, the optimizer and the evaluation metrics must be defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmnZB0CRWVPI"
      },
      "outputs": [],
      "source": [
        "# Function to define model architecture\n",
        "def build_model():\n",
        "  # Defining the architecture\n",
        "  # Sequential = Feedforward Neural Network\n",
        "  # 1 single neuron\n",
        "  # input_shape is the amount of columns from training set\n",
        "  model = keras.Sequential([\n",
        "        layers.Dense(21, input_shape = [len(train_x.columns)], activation=\"relu\"),\n",
        "        layers.Dense(14, activation=\"relu\"),\n",
        "        layers.Dense(7, activation=\"softmax\")\n",
        "  ])\n",
        "\n",
        "  # Defining the optimizer\n",
        "  optimizer = tf.keras.optimizers.RMSprop(\n",
        "      learning_rate = 0.001)\n",
        "\n",
        "  # Mean Squared Error (MSE) is the default loss function in regression models\n",
        "  model.compile(loss = 'categorical_crossentropy',\n",
        "      optimizer = optimizer,\n",
        "      metrics = ['categorical_crossentropy','accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZMgKCn8jouX"
      },
      "source": [
        "Just for curiosity, you should observe how many parameters ($\\theta$) your model has.\n",
        "At this point, your model is built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0C1tp1naRoS"
      },
      "outputs": [],
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoQyGcmFWOGS"
      },
      "source": [
        "After creating the model, it must be trained (fitted).\n",
        "Training is done using training set and the amount of epochs must be defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-Y4iU2naonL"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 200\n",
        "\n",
        "history = model.fit(\n",
        "    train_x, train_y_bin, epochs = EPOCHS, verbose = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5znM4xvHLcp"
      },
      "source": [
        "This plot should be generated just to inspect the learning convergence.\n",
        "It is expected a decreasing of the loss function value through the epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LfbEFBSHz7KC"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['categorical_crossentropy'])\n",
        "plt.title('Training Categorical Cross Entropy')\n",
        "plt.ylabel('Categorical Cross Entropy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Error'], loc='upper right')\n",
        "plt.savefig(\"trainingerror.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Training Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Error'], loc='lower right')\n",
        "plt.savefig(\"trainingaccuracy.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kcKssf0sOqFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTqSvbNeOJG5"
      },
      "source": [
        "After the training process, the knowledge learnt by a neural network is stored in its weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9ZQbtPLOfze"
      },
      "source": [
        "After the training process, the model should be tested in order to measure its quality, it means, how good are its predictions. The model must be evaluated using the testing set, which is composed by samples that are not in the training set. In regression problems, the correlation coefficient is the default metric to measure the model quality.\n",
        "The correlation coefficient is computed using real outputs ($y$) and predicted outputs ($\\hat{y}$). Correlation coefficient can vary between 0 (bad predictions) and 1 (perfect predictions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5PeLuUK06cw"
      },
      "outputs": [],
      "source": [
        "test_predictions = model.predict(test_x) # predict randon activities with the built linear regression model\n",
        "print(confusion_matrix(test_predictions.argmax(axis=1), test_y_bin.argmax(axis=1)))\n",
        "print(classification_report(test_predictions.argmax(axis=1), test_y_bin.argmax(axis=1), target_names=['red soil', 'cotton crop', 'grey soil', 'damp grey soil', 'soil with vegetation stubble', 'very damp grey soil']))\n",
        "print('The accuracy on the test set is equal to: %.4f ' % accuracy_score(test_predictions.argmax(axis=1), test_y_bin.argmax(axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(test_y_bin.argmax(axis=1), test_predictions.argmax(axis=1))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['red', 'cotton', 'grey', 'damp', 'w/veg', 'very damp'])\n",
        "disp.plot(cmap=\"bwr\")\n",
        "plt.title('Confusion Matrix for Testing Set')\n",
        "plt.savefig(\"confusionmatrix.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FCqcb_ajIXIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2EHI_hd4OdI"
      },
      "outputs": [],
      "source": [
        "train_predictions = model.predict(train_x)\n",
        "print(confusion_matrix(train_predictions.argmax(axis=1), train_y_bin.argmax(axis=1)))\n",
        "print(classification_report(train_predictions.argmax(axis=1), train_y_bin.argmax(axis=1), target_names=['red soil', 'cotton crop', 'grey soil', 'damp grey soil', 'soil with vegetation stubble', 'very damp grey soil']))\n",
        "print('The accuracy on the training set is equal to: %.4f ' % accuracy_score(train_predictions.argmax(axis=1), train_y_bin.argmax(axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install ann_visualizer\n",
        "from ann_visualizer.visualize import ann_viz\n",
        "ann_viz(model, view=True, filename=\"my_model\", title=\"Simple Architecture\")"
      ],
      "metadata": {
        "id": "hypm59Drh4xj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}