{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Federal University of Pampa <www.unipampa.edu.br>\n",
        "# Course: Deep Learning\n",
        "# Author: Sandro Camargo <sandrocamargo@unipampa.edu.br>\n",
        "# Single Neuron Logistic Regression Example\n",
        "# Dataset: https://archive.ics.uci.edu/dataset/547/algerian+forest+fires+dataset\n",
        "\n",
        "To open this code in your Google Colab environment, [click here](https://colab.research.google.com/github/Sandrocamargo/deep-learning/blob/master/dl_class03_LogisticRegressionSingleNeuron.ipynb)."
      ],
      "metadata": {
        "id": "9feqnHODNlEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Python library is a collection of related functions. A library contains bundles of encapsuated code which can be used repeatedly in different programs."
      ],
      "metadata": {
        "id": "NVnjeeftXQl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import keras # Neural Network Library\n",
        "from keras import layers # Layers to a neural network\n",
        "from keras import optimizers # optimizers\n",
        "from keras.utils import plot_model # Print the network\n",
        "import pandas as pd # Data Manipulation library\n",
        "import numpy as np # Fast Numeric Computing library\n",
        "import tensorflow as tf # Optimizers\n",
        "import matplotlib.pyplot as plt # Plot library\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "lt7-B5mnWZRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset\n",
        "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00547/Algerian_forest_fires_dataset_UPDATE.csv', header=1, skiprows=[124,125,126,170])\n",
        "# About the parameters\n",
        "# Header=1: column names (day, month, year, ...) are in the line 1 of this CSV file\n",
        "# skiprows=[124,125,126,170]: this lines, which not contains valid data, are not imported. If this parameter is missing, all lines are imported.\n",
        "\n",
        "# inspecting columns and data types from \"data\" dataframe\n",
        "data.info()"
      ],
      "metadata": {
        "id": "pBNiKzKsWtW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store target column in y\n",
        "# Store the inputs in X\n",
        "y = data[data.columns[13]]\n",
        "X = data.drop(columns=data.columns[13])"
      ],
      "metadata": {
        "id": "iWNBr7bcjLik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are whitespaces in target column, is some samples.\n",
        "print(y.value_counts())\n",
        "y = pd.Series(y)\n",
        "y = y.str.strip() # Remove whitespaces from extremes\n",
        "print(y.value_counts())"
      ],
      "metadata": {
        "id": "chQxMKCZAsc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25,  random_state=1, stratify=y)"
      ],
      "metadata": {
        "id": "75gSUBgWlGTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset must be randomly splitted in two parts: training set and testing set. The main approaches to split are holdout and n-fold cross validation.\n",
        "*   Training set is used for building (training) the model.\n",
        "*   Testing set is used for testing the generalization ability of the model built.\n",
        "\n",
        "Moreover, inputs($x$) and outputs($y$) must be splitted in each set.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tmq8UTaXdFtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying dataset dimensions\n",
        "print('The training dataset (inputs) dimensions are: ', train_x.shape)\n",
        "print('The training dataset (outputs) dimensions are: ', train_y.shape)\n",
        "print('The testing dataset (inputs) dimensions are: ', test_x.shape)\n",
        "print('The testing dataset (outputs) dimensions are: ', test_y.shape)"
      ],
      "metadata": {
        "id": "-vjSR2UWY3gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is the column Classes. Its content is 'fire' or 'not fire'. But neural networks just deal with numbers. So, classes must be transformed in a binary column containing 0 for 'not fire' and 1 for 'fire'."
      ],
      "metadata": {
        "id": "e1xTRE6ilo_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping values to binary: hot encoding\n",
        "binary_mapping = {'fire': 1, 'not fire': 0}\n",
        "\n",
        "train_y_bin = train_y.map(binary_mapping)\n",
        "test_y_bin = test_y.map(binary_mapping)"
      ],
      "metadata": {
        "id": "4nC8FyNDDAfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating the datasets, the next step is defining the architecture of our model.\n",
        "\n",
        "It must be defined:\n",
        "\n",
        "\n",
        "*   Architecture: in terms of neurons and layers\n",
        "*   Optimizer: is the algorithm or method used to change the weights in order to minimize the loss function.\n",
        "\n",
        "The last step is compiling the model. In this step the loss function, the optimizer and the evaluation metrics must be defined."
      ],
      "metadata": {
        "id": "_8NAHkCSh5sr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmnZB0CRWVPI"
      },
      "outputs": [],
      "source": [
        "# Function to define model architecture\n",
        "def build_model():\n",
        "  # Defining the architecture\n",
        "  # Sequential = Feedforward Neural Network\n",
        "  # 1 single neuron\n",
        "  # input_shape is the amount of columns from training set\n",
        "  # activation function must be sigmoid, because this is a classification problem\n",
        "  model = keras.Sequential([\n",
        "        layers.Input(shape=[len(train_x.columns)]),\n",
        "        layers.Dense(1, activation = 'sigmoid')\n",
        "  ])\n",
        "\n",
        "  # Defining the optimizer\n",
        "  optimizer = tf.keras.optimizers.RMSprop(\n",
        "      learning_rate = 0.001)\n",
        "\n",
        "  # Binary Cross Entropy is the default loss function in classification models\n",
        "  model.compile(loss = 'binary_crossentropy',\n",
        "      optimizer = optimizer,\n",
        "      metrics = ['binary_crossentropy','binary_accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just for curiosity, you should observe how many parameters ($\\theta$) your model has.\n",
        "At this point, your model is built."
      ],
      "metadata": {
        "id": "8ZMgKCn8jouX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "a0C1tp1naRoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating the model, it must be trained (fitted).\n",
        "Training is done using training set and the amount of epochs must be defined."
      ],
      "metadata": {
        "id": "SoQyGcmFWOGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 500\n",
        "\n",
        "history = model.fit(\n",
        "    train_x, train_y_bin, epochs = EPOCHS, verbose = 1\n",
        ")"
      ],
      "metadata": {
        "id": "R-Y4iU2naonL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot should be generated just to inspect the learning convergence.\n",
        "It is expected a decreasing of the loss function value through the epochs.\n"
      ],
      "metadata": {
        "id": "b5znM4xvHLcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['binary_crossentropy'])\n",
        "plt.title('Training Binary Cross Entropy')\n",
        "plt.ylabel('Binary Cross Entropy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Error'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LfbEFBSHz7KC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['binary_accuracy'])\n",
        "plt.title('Training Binary Accuracy')\n",
        "plt.ylabel('Binary Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Accuracy'], loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WBuicTtBsE6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the training process, the knowledge learnt by a neural network is stored in its weights."
      ],
      "metadata": {
        "id": "YTqSvbNeOJG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights, biases = model.get_weights() # return a numpy list of weights\n",
        "print(weights)\n",
        "plt.plot(weights)\n",
        "plt.ylabel('Weights')\n",
        "plt.xlabel('Inputs')"
      ],
      "metadata": {
        "id": "-BJawrri0hA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.barh(train_x.columns, weights[:,0].astype(float), align='center')\n",
        "plt.xlabel(\"Weights\")\n",
        "plt.ylabel(\"Inputs\")\n",
        "#plt.title(target)\n",
        "plt.savefig(\"NN-Weights.png\")"
      ],
      "metadata": {
        "id": "VRNsqSI1KeyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = model.predict(test_x).flatten() # predict radon activities with the built linear regression model\n",
        "test_predictions1 = test_predictions > 0.5 #np.around(test_predictions)  # > 0.5\n",
        "tp = np.count_nonzero((test_predictions1 == 1) & (np.transpose(test_y_bin) == 1))\n",
        "tn = np.count_nonzero((test_predictions1 == 0) & (np.transpose(test_y_bin) == 0))\n",
        "accuracy_test = (tp + tn)/len(test_y)\n",
        "print('The accuracy on the test set is equal to: %.4f %%' % (accuracy_test*100))"
      ],
      "metadata": {
        "id": "gEESHuWZL-XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate classification report\n",
        "print(classification_report(test_y_bin, test_predictions1, target_names=[\"Not Fire\", \"Fire\"]))"
      ],
      "metadata": {
        "id": "ss7laupuz9cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions = model.predict(train_x).flatten() # predict radon activities with the built linear regression model\n",
        "train_predictions1 = train_predictions > 0.5\n",
        "tp = np.count_nonzero((train_predictions1 == 1) & (np.transpose(train_y_bin) == 1))\n",
        "tn = np.count_nonzero((train_predictions1 == 0) & (np.transpose(train_y_bin) == 0))\n",
        "accuracy_train = (tp + tn)/len(train_y)\n",
        "print('The accuracy on the training set is equal to: %.4f %%.' % (accuracy_train*100))"
      ],
      "metadata": {
        "id": "kq1Thn2Ib5cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate classification report\n",
        "print(classification_report(train_y_bin, train_predictions1, target_names=[\"Not Fire\", \"Fire\"]))"
      ],
      "metadata": {
        "id": "_hw3ifKn0Qxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the training process, the model should be tested in order to measure its quality, it means, how good are its predictions. The model must be evaluated using the testing set, which is composed by samples that are not in the training set. In classification problems, the accuracy is the default metric.\n",
        "The accuracy is computed using real outputs ($y$) and predicted outputs ($\\hat{y}$). Accuracy can vary between 0 (bad predictions) and 1 (perfect predictions). Accuracy also can be presented in percentage, ranging from 0 to 100%."
      ],
      "metadata": {
        "id": "J9ZQbtPLOfze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model architecture as an image\n",
        "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "ZGA1306r7GTM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}