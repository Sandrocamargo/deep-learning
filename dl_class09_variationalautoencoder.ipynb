{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Course: Deep Learning\n",
        "# Author: Sandro Camargo sandrocamargo@unipampa.edu.br\n",
        "# Variational Autoencoder (VAE) for MNIST\n",
        "\n",
        "Dataset: '/content/sample_data/' mnist\n",
        "\n",
        "Dataset description: https://www.tensorflow.org/datasets/catalog/mnist\n",
        "\n",
        "To open this code in your Google Colab environment, [click here](https://colab.research.google.com/github/Sandrocamargo/deep-learning/blob/master/dl_class09_variationalautoencoder.ipynb).\n"
      ],
      "metadata": {
        "id": "3-_h1i_Ln6BN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading libraries"
      ],
      "metadata": {
        "id": "fekyjgDzoEIS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlypMTDLfuum"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dataset"
      ],
      "metadata": {
        "id": "vf6w-IvSoI33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.expand_dims(x_train, -1)"
      ],
      "metadata": {
        "id": "uoU7P3Df-7g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting variational autoencoder"
      ],
      "metadata": {
        "id": "Lw7bk19ToNPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 2   # muito melhor para CNN\n",
        "\n",
        "# ===========================================\n",
        "# ENCODER\n",
        "# ===========================================\n",
        "inputs = layers.Input(shape=(28,28,1))\n",
        "\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "\n",
        "z_mean = layers.Dense(latent_dim)(x)\n",
        "z_log_var = layers.Dense(latent_dim)(x)\n",
        "\n",
        "def sampling(args):\n",
        "    zm, zv = args\n",
        "    eps = tf.random.normal(shape=(tf.shape(zm)[0], latent_dim))\n",
        "    return zm + tf.exp(0.5 * zv) * eps\n",
        "\n",
        "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z])\n",
        "\n",
        "# ===========================================\n",
        "# DECODER\n",
        "# ===========================================\n",
        "latent_inputs = layers.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7*7*64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((7,7,64))(x)\n",
        "\n",
        "x = layers.Conv2DTranspose(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "\n",
        "decoder = Model(latent_inputs, outputs)\n",
        "\n",
        "# ===========================================\n",
        "# VAE class\n",
        "# ===========================================\n",
        "class VAE(Model):\n",
        "    def __init__(self, encoder, decoder, beta=1.0):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.beta = beta\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "\n",
        "            # BCE por pixel (muito melhor para MNIST)\n",
        "            recon_loss = tf.reduce_sum(\n",
        "                tf.keras.losses.binary_crossentropy(data, reconstruction),\n",
        "                axis=(1,2)\n",
        "            )\n",
        "\n",
        "            kl_loss = -0.5 * tf.reduce_sum(\n",
        "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var),\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "            loss = tf.reduce_mean(recon_loss + self.beta * kl_loss)\n",
        "\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "\n",
        "        return {\"loss\": loss, \"recon\": tf.reduce_mean(recon_loss), \"kl\": tf.reduce_mean(kl_loss)}\n",
        "\n",
        "vae = VAE(encoder, decoder, beta=1.0)"
      ],
      "metadata": {
        "id": "UCymK-3I--MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compiling and training"
      ],
      "metadata": {
        "id": "bXePKxy-oQW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae.compile(optimizer=tf.keras.optimizers.Adam(1e-3))\n",
        "vae.summary()\n",
        "\n",
        "# ===========================================\n",
        "# Treinamento\n",
        "# ===========================================\n",
        "history = vae.fit(x_train, epochs=50, batch_size=128)"
      ],
      "metadata": {
        "id": "sG8rTSh3_DaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Viewing loss"
      ],
      "metadata": {
        "id": "18VLEGT5VKJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['recon'])\n",
        "plt.title('Loss Function')\n",
        "plt.ylabel('Categorical Cross Entropy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training Loss','Reconstruction Loss'], loc='upper right')\n",
        "plt.savefig(\"trainingerror.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0mwENb4E_LlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['kl'])\n",
        "plt.title('KL Function')\n",
        "plt.ylabel('Categorical Cross Entropy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['KL'], loc='lower right')\n",
        "plt.savefig(\"trainingkl.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZmfNQpRW42uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Viewing results"
      ],
      "metadata": {
        "id": "hLjzbhfsoW6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# Generate 100 images\n",
        "# ===========================================\n",
        "\n",
        "import os\n",
        "os.makedirs(\"synthetic_digits\", exist_ok=True)\n",
        "\n",
        "for digit in range(10):\n",
        "    print(f\"Gerando imagens ...\")\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(15, 2))\n",
        "\n",
        "    for i in range(10):\n",
        "        # Sample from latent space\n",
        "        z_sample = np.random.normal(size=(1, latent_dim))\n",
        "        img = decoder(z_sample).numpy().reshape(28, 28)\n",
        "\n",
        "        axes[i].imshow(img, cmap=\"gray\")\n",
        "        axes[i].axis(\"off\")\n",
        "\n",
        "        # Save file\n",
        "        plt.imsave(f\"synthetic_digits/{digit}_{i}.png\", img, cmap=\"gray\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "print(\"Imagens salvas em synthetic_digits/\")"
      ],
      "metadata": {
        "id": "DyXnMW9F_OUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}