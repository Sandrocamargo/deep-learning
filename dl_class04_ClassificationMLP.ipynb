{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9feqnHODNlEV"
      },
      "source": [
        "# Course: Deep Learning\n",
        "# Author: Sandro Camargo <sandrocamargo@unipampa.edu.br>\n",
        "# Classification with Multi Layer Perceptron Example Urban Land Cover\n",
        "# Dataset: https://archive.ics.uci.edu/ml/datasets/Urban+Land+Cover"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVnjeeftXQl6"
      },
      "source": [
        "A Python library is a collection of related functions. A library contains bundles of encapsulated code which can be used repeatedly in different programs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt7-B5mnWZRb"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import keras # Neural Network Library\n",
        "from keras import layers # Layers to a neural network\n",
        "from keras import optimizers # optimizers\n",
        "import pandas as pd # Data Manipulation library\n",
        "import numpy as np # Fast Numeric Computing library\n",
        "import tensorflow as tf # Optimizers\n",
        "import matplotlib.pyplot as plt # Plot library\n",
        "from sklearn.preprocessing import MinMaxScaler, label_binarize\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.utils import plot_model # Print the network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00295/Urban%20land%20cover.zip\", \"urbanlandcover.zip\")"
      ],
      "metadata": {
        "id": "-GBAxPnYnm_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"urbanlandcover.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")"
      ],
      "metadata": {
        "id": "GB1X3Jtcn428"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBNiKzKsWtW9"
      },
      "outputs": [],
      "source": [
        "# Loading training dataset\n",
        "data = pd.read_csv('training.csv', delimiter=\",\", header=0)\n",
        "# About the parameters\n",
        "# Header=1: column names (day, month, year, ...) are in the line 1 of this CSV file.\n",
        "# skiprows=[124,125,126,170]: this lines, which not contains valid data, are not imported. If this parameter is missing, all lines are imported.\n",
        "# usecols=list(range(0,13)): The last column, which is named Classes, is not imported. If this parameter is missing, all columns are imported.\n",
        "\n",
        "# inspecting columns and data types from \"data\" dataframe\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datatest = pd.read_csv('testing.csv', delimiter=\",\", header=0)\n",
        "datatest.info()"
      ],
      "metadata": {
        "id": "cexznDgEqlWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = data.iloc[:,0]\n",
        "data.drop(data.columns[[0]], axis=1, inplace=True)\n",
        "scaler = MinMaxScaler()\n",
        "print(scaler.fit(data))\n",
        "MinMaxScaler()\n",
        "data = pd.DataFrame(scaler.transform(data))"
      ],
      "metadata": {
        "id": "I2gEa1Jtq7Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classestest = datatest.iloc[:,0]\n",
        "datatest.drop(datatest.columns[[0]], axis=1, inplace=True)\n",
        "scalertest = MinMaxScaler()\n",
        "print(scalertest.fit(datatest))\n",
        "MinMaxScaler()\n",
        "datatest = pd.DataFrame(scaler.transform(datatest))"
      ],
      "metadata": {
        "id": "B8w9bBdnq_PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmq8UTaXdFtF"
      },
      "source": [
        "The dataset must be randomly splitted in two parts: training set and testing set. The main approaches to split are holdout and n-fold cross validation.\n",
        "*   Training set is used for building (training) the model.\n",
        "*   Testing set is used for testing the generalization ability of the model built.\n",
        "\n",
        "Moreover, inputs($x$) and outputs($y$) must be splitted in each set.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vjSR2UWY3gi"
      },
      "outputs": [],
      "source": [
        "# Creating the training dataset\n",
        "train_x = data\n",
        "train_y = label_binarize(classes, classes=['tree ', 'grass ', 'soil ', 'concrete ', 'asphalt ', 'building ', 'car ', 'pool ', 'shadow '])\n",
        "\n",
        "# Creating the testing dataset\n",
        "test_x = datatest\n",
        "test_y = label_binarize(classestest, classes=['tree ', 'grass ', 'soil ', 'concrete ', 'asphalt ', 'building ', 'car ', 'pool ', 'shadow '])\n",
        "\n",
        "# Verifying dataset dimensions\n",
        "print('The training dataset (inputs) dimensions are: ', train_x.shape)\n",
        "print('The training dataset (outputs) dimensions are: ', train_y.shape)\n",
        "print('The testing dataset (inputs) dimensions are: ', test_x.shape)\n",
        "print('The testing dataset (outputs) dimensions are: ', test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8NAHkCSh5sr"
      },
      "source": [
        "After creating the datasets, the next step is defining the architecture of our model.\n",
        "\n",
        "It must be defined:\n",
        "\n",
        "\n",
        "*   Architecture: in terms of neurons and layers\n",
        "*   Optimizer: is the algorithm or method used to change the weights in order to minimize the loss function.\n",
        "\n",
        "The last step is compiling the model. In this step the loss function, the optimizer and the evaluation metrics must be defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmnZB0CRWVPI"
      },
      "outputs": [],
      "source": [
        "# Function to define model architecture\n",
        "def build_model():\n",
        "  # Defining the architecture\n",
        "  # Sequential = Feedforward Neural Network\n",
        "  # 1 single neuron\n",
        "  # input_shape is the amount of columns from training set\n",
        "  model = keras.Sequential([\n",
        "        layers.Input(shape=[len(train_x.columns)]),\n",
        "        layers.Dense(9, activation=\"relu\"),\n",
        "        layers.Dense(9, activation=\"softmax\")\n",
        "  ])\n",
        "\n",
        "  # Defining the optimizer\n",
        "  optimizer = tf.keras.optimizers.RMSprop(\n",
        "      learning_rate = 0.001)\n",
        "\n",
        "  # Mean Squared Error (MSE) is the default loss function in regression models\n",
        "  model.compile(loss = 'categorical_crossentropy',\n",
        "      optimizer = optimizer,\n",
        "      metrics = ['categorical_crossentropy','accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZMgKCn8jouX"
      },
      "source": [
        "Just for curiosity, you should observe how many parameters ($\\theta$) your model has.\n",
        "At this point, your model is built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0C1tp1naRoS"
      },
      "outputs": [],
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoQyGcmFWOGS"
      },
      "source": [
        "After creating the model, it must be trained (fitted).\n",
        "Training is done using training set and the amount of epochs must be defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-Y4iU2naonL"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "history = model.fit(\n",
        "    train_x, train_y, epochs = EPOCHS, verbose = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5znM4xvHLcp"
      },
      "source": [
        "This plot should be generated just to inspect the learning convergence.\n",
        "It is expected a decreasing of the loss function value through the epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LfbEFBSHz7KC"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['categorical_crossentropy'])\n",
        "plt.title('Training Categorical Cross Entropy')\n",
        "plt.ylabel('Categorical Cross Entropy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Error'], loc='upper right')\n",
        "plt.savefig(\"mlp-class-lossfunction.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Training Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Accuracy'], loc='lower right')\n",
        "plt.savefig(\"mlp-class-trainingaccuracy.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kcKssf0sOqFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model.get_weights() # return a numpy list of weights\n",
        "print(type(weights))\n",
        "print(weights)"
      ],
      "metadata": {
        "id": "HgS78MBRyfbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTqSvbNeOJG5"
      },
      "source": [
        "After the training process, the knowledge learnt by a neural network is stored in its weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9ZQbtPLOfze"
      },
      "source": [
        "After the training process, the model should be tested in order to measure its quality, it means, how good are its predictions. The model must be evaluated using the testing set, which is composed by samples that are not in the training set. In regression problems, the correlation coefficient is the default metric to measure the model quality.\n",
        "The correlation coefficient is computed using real outputs ($y$) and predicted outputs ($\\hat{y}$). Correlation coefficient can vary between 0 (bad predictions) and 1 (perfect predictions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5PeLuUK06cw"
      },
      "outputs": [],
      "source": [
        "test_predictions = model.predict(test_x) # predict randon activities with the built linear regression model\n",
        "print(confusion_matrix(test_predictions.argmax(axis=1), test_y.argmax(axis=1)))\n",
        "print(classification_report(test_predictions.argmax(axis=1), test_y.argmax(axis=1), target_names=['tree ', 'grass ', 'soil ', 'concrete ', 'asphalt ', 'building ', 'car ', 'pool ', 'shadow ']))\n",
        "print('The accuracy on the test set is equal to: %.4f ' % accuracy_score(test_predictions.argmax(axis=1), test_y.argmax(axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(test_predictions.argmax(axis=1), test_y.argmax(axis=1))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['tree', 'grass', 'soil', 'conc', 'asph', 'build', 'car', 'pool', 'shad'])\n",
        "disp.plot(cmap=\"bwr\")\n",
        "plt.title('Confusion Matrix for Testing Set')\n",
        "plt.savefig(\"mlp-class-confusionmatrix.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FCqcb_ajIXIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2EHI_hd4OdI"
      },
      "outputs": [],
      "source": [
        "train_predictions = model.predict(train_x)\n",
        "print(confusion_matrix(train_predictions.argmax(axis=1), train_y.argmax(axis=1)))\n",
        "print(classification_report(train_predictions.argmax(axis=1), train_y.argmax(axis=1), target_names=['tree ', 'grass ', 'soil ', 'concrete ', 'asphalt ', 'building ', 'car ', 'pool ', 'shadow ']))\n",
        "print('The accuracy on the training set is equal to: %.4f ' % accuracy_score(train_predictions.argmax(axis=1), train_y.argmax(axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model architecture as an image\n",
        "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "CmLt-5ERpFQN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}