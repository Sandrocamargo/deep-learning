{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9feqnHODNlEV"
      },
      "source": [
        "# Course: Deep Learning\n",
        "# Author: Sandro Camargo <sandrocamargo@unipampa.edu.br>\n",
        "# Non Linear Regression with Multi Layer Perceptron Example\n",
        "# Dataset: https://archive.ics.uci.edu/ml/datasets/Algerian+Forest+Fires+Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVnjeeftXQl6"
      },
      "source": [
        "A Python library is a collection of related functions. A library contains bundles of encapsulated code which can be used repeatedly in different programs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt7-B5mnWZRb"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import keras # Neural Network Library\n",
        "from keras import layers # Layers to a neural network\n",
        "from keras import optimizers # optimizers\n",
        "import pandas as pd # Data Manipulation library\n",
        "import numpy as np # Fast Numeric Computing library\n",
        "import tensorflow as tf # Optimizers\n",
        "import matplotlib.pyplot as plt # Plot library\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.utils import plot_model # Print the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBNiKzKsWtW9"
      },
      "outputs": [],
      "source": [
        "# Loading dataset\n",
        "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00547/Algerian_forest_fires_dataset_UPDATE.csv', header=1, skiprows=[124,125,126,170], usecols=list(range(0,13)))\n",
        "# About the parameters\n",
        "# Header=1: column names (day, month, year, ...) are in the line 1 of this CSV file.\n",
        "# skiprows=[124,125,126,170]: this lines, which not contains valid data, are not imported. If this parameter is missing, all lines are imported.\n",
        "# usecols=list(range(0,13)): The last column, which is named Classes, is not imported. If this parameter is missing, all columns are imported.\n",
        "\n",
        "# inspecting columns and data types from \"data\" dataframe\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2gEa1Jtq7Jh"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "print(scaler.fit(data))\n",
        "MinMaxScaler()\n",
        "data = pd.DataFrame(scaler.transform(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmq8UTaXdFtF"
      },
      "source": [
        "The dataset must be randomly splitted in two parts: training set and testing set. The main approaches to split are holdout and n-fold cross validation.\n",
        "*   Training set is used for building (training) the model.\n",
        "*   Testing set is used for testing the generalization ability of the model built.\n",
        "\n",
        "Moreover, inputs($x$) and outputs($y$) must be splitted in each set.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vjSR2UWY3gi"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1) # Random numbers will be ever the same\n",
        "rnd = np.random.rand(len(data)) < 0.8 # Training set will contain 80% of the data\n",
        "\n",
        "# Create the training dataset (80%)\n",
        "train_data = data[rnd].copy()\n",
        "train_x = train_data.drop(columns=data.columns[12])  # Drop the output column\n",
        "train_y = train_data[data.columns[12]]  # Extract the output column\n",
        "\n",
        "# Create the testing dataset (20%)\n",
        "test_data = data[~rnd].copy()\n",
        "test_x = test_data.drop(columns=data.columns[12])  # Drop the output column\n",
        "test_y = test_data[data.columns[12]]  # Extract the output column\n",
        "\n",
        "# Creating the training dataset (80%)\n",
        "#train_x = data[rnd]\n",
        "#train_x.drop(train_x.columns[[12]], axis=1, inplace=True) # column 12 is removed, because it is the output (y)\n",
        "#train_y = data[rnd]\n",
        "#train_y.drop(train_y.iloc[:, 0:12], axis=1, inplace=True) # columns from 0 to 11 are removed, because they are the inputs (x)\n",
        "\n",
        "# Creating the testing dataset (20%)\n",
        "#test_x = data[~rnd]\n",
        "#test_x.drop(test_x.columns[[12]], axis=1, inplace=True)\n",
        "#test_y = data[~rnd]\n",
        "#test_y.drop(test_y.iloc[:, 0:12], axis=1, inplace=True)\n",
        "\n",
        "# Verifying dataset dimensions\n",
        "print('The training dataset (inputs) dimensions are: ', train_x.shape)\n",
        "print('The training dataset (outputs) dimensions are: ', train_y.shape)\n",
        "print('The testing dataset (inputs) dimensions are: ', test_x.shape)\n",
        "print('The testing dataset (outputs) dimensions are: ', test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8NAHkCSh5sr"
      },
      "source": [
        "After creating the datasets, the next step is defining the architecture of our model.\n",
        "\n",
        "It must be defined:\n",
        "\n",
        "\n",
        "*   Architecture: in terms of neurons and layers\n",
        "*   Optimizer: is the algorithm or method used to change the weights in order to minimize the loss function.\n",
        "\n",
        "The last step is compiling the model. In this step the loss function, the optimizer and the evaluation metrics must be defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmnZB0CRWVPI"
      },
      "outputs": [],
      "source": [
        "# Function to define model architecture\n",
        "def build_model():\n",
        "  # Defining the architecture\n",
        "  # Sequential = Feedforward Neural Network\n",
        "  # input_shape is the amount of columns from training set\n",
        "  model = keras.Sequential([\n",
        "        layers.Input(shape=[len(train_x.columns)]),\n",
        "        layers.Dense(10, activation=\"relu\"),\n",
        "        layers.Dense(5, activation=\"relu\"),\n",
        "        layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  # Defining the optimizer\n",
        "  optimizer = tf.keras.optimizers.RMSprop(\n",
        "      learning_rate = 0.001)\n",
        "\n",
        "  # Mean Squared Error (MSE) is the default loss function in regression models\n",
        "  model.compile(loss = 'mse',\n",
        "      optimizer = optimizer,\n",
        "      metrics = ['mse','mae'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZMgKCn8jouX"
      },
      "source": [
        "Just for curiosity, you should observe how many parameters ($\\theta$) your model has.\n",
        "At this point, your model is built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0C1tp1naRoS"
      },
      "outputs": [],
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoQyGcmFWOGS"
      },
      "source": [
        "After creating the model, it must be trained (fitted).\n",
        "Training is done using training set and the amount of epochs must be defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-Y4iU2naonL"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 200\n",
        "\n",
        "history = model.fit(\n",
        "    train_x, train_y, epochs = EPOCHS, verbose = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5znM4xvHLcp"
      },
      "source": [
        "This plot should be generated just to inspect the learning convergence.\n",
        "It is expected a decreasing of the loss function value through the epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LfbEFBSHz7KC"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['mse'])\n",
        "plt.title('Training MSE')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Error'], loc='upper right')\n",
        "plt.savefig(\"mlp-regr-lossfunction.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs2zotw_jm60"
      },
      "outputs": [],
      "source": [
        "weights = model.get_weights() # return a numpy list of weights\n",
        "print(type(weights))\n",
        "print(weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTqSvbNeOJG5"
      },
      "source": [
        "After the training process, the knowledge learnt by a neural network is stored in its weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9ZQbtPLOfze"
      },
      "source": [
        "After the training process, the model should be tested in order to measure its quality, it means, how good are its predictions. The model must be evaluated using the testing set, which is composed by samples that are not in the training set. In regression problems, the correlation coefficient is the default metric to measure the model quality.\n",
        "The correlation coefficient is computed using real outputs ($y$) and predicted outputs ($\\hat{y}$). Correlation coefficient can vary between 0 (bad predictions) and 1 (perfect predictions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5PeLuUK06cw"
      },
      "outputs": [],
      "source": [
        "test_predictions = model.predict(test_x) # predict radon activities with the built linear regression model\n",
        "\n",
        "plt.scatter(test_y, test_predictions, marker = 'o', c = 'blue')\n",
        "plt.plot([-0.1,1.1], [-0.1,1.1], color = 'black', ls = '--')\n",
        "plt.ylabel('Predictions')\n",
        "plt.xlabel('Real Values')\n",
        "plt.title('Regression with MLP (Testing Set)')\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.xlim(-0.1, 1.1)\n",
        "plt.axis(True)\n",
        "plt.savefig(\"mlp-regr-testing.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Correlation Coefficient in testing set: %.4f\" % np.corrcoef(np.transpose(test_predictions), np.transpose(test_y))[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2EHI_hd4OdI"
      },
      "outputs": [],
      "source": [
        "train_predictions = model.predict(train_x) # predict radom activities with the built linear regression model\n",
        "\n",
        "plt.scatter(train_y, train_predictions, marker = 'o', c = 'blue')\n",
        "plt.plot([-0.1,1.1], [-0.1,1.1], color = 'black', ls = '--')\n",
        "plt.ylabel('Predictions')\n",
        "plt.xlabel('Real Values')\n",
        "plt.title('Regression with MLP (Training Set)')\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.xlim(-0.1, 1.1)\n",
        "plt.axis(True)\n",
        "plt.savefig(\"mlp-regr-training.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Correlation Coefficient in training set: %.4f\" % np.corrcoef(np.transpose(train_predictions), np.transpose(train_y))[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hypm59Drh4xj"
      },
      "outputs": [],
      "source": [
        "# Save the model architecture as an image\n",
        "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}